{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow项目选拔\n",
    "\n",
    "本次选拔采用优秀者优先的规则，内容是实现TensorFlow相关项目来选拔优秀者。由于项目比较重要，请同学首先考虑是否有足够的时间在8月底之前一起合作完成。\n",
    "\n",
    "**使用工具：Jupyter Notebook，python 3.0，TensorFlow**\n",
    "\n",
    "**任务介绍：**\n",
    "\n",
    "认真阅读之前在群里发的教学大纲的文件，任务是**基于Tensor Flow的深度学习框架实践**这个板块中第二个算法来实现。\n",
    "\n",
    "具体任务：熟练掌握基于**TensorFlow**的**深度卷积神经网络的原理**、实现分类网络**LeNet5**，并在手写数字识别数据集**MNIST**上**验证**和**评价**LeNet5的**分类性能**。\n",
    "\n",
    "**实现主要内容：**\n",
    "\n",
    "**1）数据集的介绍及初步探索，数据预处理主要步骤并解释处理的原因和基本方法。（文字描述部分都用md格式在Jupyter Notebook的cell里面写出来）**\n",
    "\n",
    "**2）算法原理介绍**\n",
    "\n",
    "**3）实现算法的大致描述（介绍该算法的思路）**\n",
    "\n",
    "**4）验证并简要分析评价算法得出的结果**\n",
    "\n",
    "**5）相关书籍，论文或者博客参考地址（Reference）**\n",
    "\n",
    "**Tips：**课程目标人群是大二的学生，所以可以尽量把基础，实现步骤解释详细一点，实践难度可以不用太高。\n",
    "\n",
    "**DDL: 20190728 Submit the [Github](https://github.com/Imay-King/CUPTProject)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" text=\"图一\" style=\"padding-left:1px;\" src=\"res/全连接与卷积神经网络.png\">\n",
    "\n",
    "> **图 1. 全连接神经网络与卷积神经网络结构图**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" text=\"图二\" style=\"padding-left:1px;\" src=\"res/卷积神经网络.png\">\n",
    "\n",
    "> **图 2. 卷积神经网络架构图**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" text=\"图一\" style=\"padding-left:1px;\" src=\"res/CNN__LeNet-5.png\">\n",
    "\n",
    "> **图 3. LeNet-5**模型结构图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA探索性数据分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**数据处理过程**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "定义需要使用到的常量\n",
    "'''\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import os.path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import gfile\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import LeNet5_infernece\n",
    "import mnist_inference\n",
    "import mnist_train\n",
    "\n",
    "# 原始输入数据的目录，这个目录下有5个子目录，每个子目录底下保存这属于该\n",
    "# 类别的所有图片。\n",
    "INPUT_DATA = '../../datasets/flower_photos'\n",
    "# 输出文件地址。我们将整理后的图片数据通过numpy的格式保存。\n",
    "OUTPUT_FILE = '../../datasets/flower_processed_data.npy'\n",
    "\n",
    "# 测试数据和验证数据比例。\n",
    "VALIDATION_PERCENTAGE = 10\n",
    "TEST_PERCENTAGE = 10\n",
    "\n",
    "'''\n",
    "定义数据处理过程\n",
    "'''\n",
    "# 读取数据并将数据分割成训练数据、验证数据和测试数据。\n",
    "def create_image_lists(sess, testing_percentage, validation_percentage):\n",
    "    sub_dirs = [x[0] for x in os.walk(INPUT_DATA)]\n",
    "    is_root_dir = True\n",
    "    \n",
    "    # 初始化各个数据集。\n",
    "    training_images = []\n",
    "    training_labels = []\n",
    "    testing_images = []\n",
    "    testing_labels = []\n",
    "    validation_images = []\n",
    "    validation_labels = []\n",
    "    current_label = 0\n",
    "    \n",
    "    # 读取所有的子目录。\n",
    "    for sub_dir in sub_dirs:\n",
    "        if is_root_dir:\n",
    "            is_root_dir = False\n",
    "            continue\n",
    "\n",
    "        # 获取一个子目录中所有的图片文件。\n",
    "        extensions = ['jpg', 'jpeg', 'JPG', 'JPEG']\n",
    "        file_list = []\n",
    "        dir_name = os.path.basename(sub_dir)\n",
    "        for extension in extensions:\n",
    "            file_glob = os.path.join(INPUT_DATA, dir_name, '*.' + extension)\n",
    "            file_list.extend(glob.glob(file_glob))\n",
    "        if not file_list: continue\n",
    "        print(\"processing:\", dir_name)\n",
    "        \n",
    "        i = 0\n",
    "        # 处理图片数据。\n",
    "        for file_name in file_list:\n",
    "            i += 1\n",
    "            # 读取并解析图片，将图片转化为299*299以方便inception-v3模型来处理。\n",
    "            image_raw_data = gfile.FastGFile(file_name, 'rb').read()\n",
    "            image = tf.image.decode_jpeg(image_raw_data)\n",
    "            if image.dtype != tf.float32:\n",
    "                image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "            image = tf.image.resize_images(image, [299, 299])\n",
    "            image_value = sess.run(image)\n",
    "            \n",
    "            # 随机划分数据聚。\n",
    "            chance = np.random.randint(100)\n",
    "            if chance < validation_percentage:\n",
    "                validation_images.append(image_value)\n",
    "                validation_labels.append(current_label)\n",
    "            elif chance < (testing_percentage + validation_percentage):\n",
    "                testing_images.append(image_value)\n",
    "                testing_labels.append(current_label)\n",
    "            else:\n",
    "                training_images.append(image_value)\n",
    "                training_labels.append(current_label)\n",
    "            if i % 200 == 0:\n",
    "                print(i, \"images processed.\")\n",
    "        current_label += 1\n",
    "    \n",
    "    # 将训练数据随机打乱以获得更好的训练效果。\n",
    "    state = np.random.get_state()\n",
    "    np.random.shuffle(training_images)\n",
    "    np.random.set_state(state)\n",
    "    np.random.shuffle(training_labels)\n",
    "    \n",
    "    return np.asarray([training_images, training_labels,\n",
    "                       validation_images, validation_labels,\n",
    "                       testing_images, testing_labels])\n",
    "\n",
    "'''\n",
    "运行数据处理过程\n",
    "'''\n",
    "with tf.Session() as sess:\n",
    "    processed_data = create_image_lists(sess, TEST_PERCENTAGE, VALIDATION_PERCENTAGE)\n",
    "    # 通过numpy格式保存处理后的数据。\n",
    "    np.save(OUTPUT_FILE, processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MNISt数据处理**\n",
    "\n",
    "[**MNIST**](<http://yann.lecun.com/exdb/mnist/>)是一个手写体数字识别数据集，其包含了60000张图片作为训练数据，10000张图片作为测试数据。在MNIST 数据集中的每一张图片都代表了0～9 中的一个数字，图片的大小都为28 * 28。而TensorFlow对MNIST数据集做了很好地封装。TensorFlow提供了一个类来处理MNIST数据。这个类会自动下载并转化MNIST 数据的格式，将数据从原始的数据包中解析成训练和测试神经网络时使用的格式为28 * 28，同时TensorFlow会自动将MNIST数据集划分为训练集、验证集和测试集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "'''\n",
    "training dataset 和validating dataset组成了MNIST本身提供的训练数据集\n",
    "'''\n",
    "# 通过input_data.read_data_sets函数生成的类会自动将MNIST数据集划分为train 、validation 和test\n",
    "mnist = input_data.read_data_sets(\"../../datasets/MNIST_data/\", one_hot=True)\n",
    "# 训练集含有的图片数\n",
    "print(\"Training dataset size: \", mnist.train.num_examples)\n",
    "# 验证集含有的图片数\n",
    "print(\"Validating dataset size: \", mnist.validation.num_examples)\n",
    "# 测试集含有的图片数\n",
    "print(\"Testing dataset size: \", mnist.test.num_examples)\n",
    "# 处理后的每张图片为一个长度为784(28*28)的一维数\n",
    "print(\"Example training data: \", mnist.train.images[0])\n",
    "print(\"Example training dataset label: \", mnist.train.labels[0])\n",
    "\n",
    "batch_size = 100\n",
    "# mnist.train.next_batch可以从所有的训练数据中读取一小部分(batch_size)作为一个训练批次\n",
    "xs, ys = mnist.train.next_batch(batch_size)\n",
    "print(\"X shape:\", xs.shape)                   \n",
    "print(\"Y shape:\", ys.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MNIST推理过程**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "定义神经网络结构相关的参数\n",
    "'''\n",
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "LAYER1_NODE = 500\n",
    "\n",
    "'''\n",
    "通过tf.get_variable函数来获取变量\n",
    "'''\n",
    "def get_weight_variable(shape, regularizer):\n",
    "    weights = tf.get_variable(\"weights\", shape, initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    if regularizer != None: tf.add_to_collection('losses', regularizer(weights))\n",
    "    return weights\n",
    "\n",
    "'''\n",
    "定义神经网络的前向传播过程\n",
    "'''\n",
    "def inference(input_tensor, regularizer):\n",
    "    with tf.variable_scope('layer1'):\n",
    "\n",
    "        weights = get_weight_variable([INPUT_NODE, LAYER1_NODE], regularizer)\n",
    "        biases = tf.get_variable(\"biases\", [LAYER1_NODE], initializer=tf.constant_initializer(0.0))\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights) + biases)\n",
    "\n",
    "    with tf.variable_scope('layer2'):\n",
    "        weights = get_weight_variable([LAYER1_NODE, OUTPUT_NODE], regularizer)\n",
    "        biases = tf.get_variable(\"biases\", [OUTPUT_NODE], initializer=tf.constant_initializer(0.0))\n",
    "        layer2 = tf.matmul(layer1, weights) + biases\n",
    "\n",
    "    return layer2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MNIST训练过程**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "定义神经网络结构相关的参数\n",
    "'''\n",
    "BATCH_SIZE = 100 \n",
    "LEARNING_RATE_BASE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 30000\n",
    "MOVING_AVERAGE_DECAY = 0.99 \n",
    "MODEL_SAVE_PATH = \"MNIST_model/\"\n",
    "MODEL_NAME = \"mnist_model\"\n",
    "\n",
    "'''\n",
    "定义训练过程\n",
    "'''\n",
    "def train(mnist):\n",
    "    # 定义输入输出placeholder。\n",
    "    x = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name='y-input')\n",
    "\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    y = mnist_inference.inference(x, regularizer)\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # 定义损失函数、学习率、滑动平均操作以及训练过程。\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        mnist.train.num_examples / BATCH_SIZE, LEARNING_RATE_DECAY,\n",
    "        staircase=True)\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "        \n",
    "    # 初始化TensorFlow持久化类。\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n",
    "            if i % 1000 == 0:\n",
    "                print(\"After %d training step(s), loss on training batch is %g.\" % (step, loss_value))\n",
    "                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)\n",
    "  \n",
    "'''\n",
    "main主函数入口\n",
    "'''\n",
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets(\"../../../datasets/MNIST_data\", one_hot=True)\n",
    "    train(mnist)\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MNIST评估过程**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "每10秒加载一次最新的模型\n",
    "'''\n",
    "# 加载的时间间隔。\n",
    "EVAL_INTERVAL_SECS = 10\n",
    "\n",
    "def evaluate(mnist):\n",
    "    with tf.Graph().as_default() as g:\n",
    "        x = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], name='x-input')\n",
    "        y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name='y-input')\n",
    "        validate_feed = {x: mnist.validation.images, y_: mnist.validation.labels}\n",
    "\n",
    "        y = mnist_inference.inference(x, None)\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        variable_averages = tf.train.ExponentialMovingAverage(mnist_train.MOVING_AVERAGE_DECAY)\n",
    "        variables_to_restore = variable_averages.variables_to_restore()\n",
    "        saver = tf.train.Saver(variables_to_restore)\n",
    "\n",
    "        while True:\n",
    "            with tf.Session() as sess:\n",
    "                ckpt = tf.train.get_checkpoint_state(mnist_train.MODEL_SAVE_PATH)\n",
    "                if ckpt and ckpt.model_checkpoint_path:\n",
    "                    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                    global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "                    accuracy_score = sess.run(accuracy, feed_dict=validate_feed)\n",
    "                    print(\"After %s training step(s), validation accuracy = %g\" % (global_step, accuracy_score))\n",
    "                else:\n",
    "                    print('No checkpoint file found')\n",
    "                    return\n",
    "            time.sleep(EVAL_INTERVAL_SECS)\n",
    "\n",
    "'''\n",
    "main主函数入口\n",
    "'''\n",
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets(\"../../../datasets/MNIST_data\", one_hot=True)\n",
    "    evaluate(mnist)\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算法原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**深度卷积神经网络**\n",
    "\n",
    "卷积神经网络与全连接神经网络的整体架构是很相似的\n",
    "\n",
    "- 卷积神经网络通过一层一层的节点组织起来，其中的每一个节点都是神经元；\n",
    "\n",
    "- 在图像分类中，卷积神经网络的输入层就是图像的原始像素，而输出层中的每一个节点代表了不同类别的可信度。这与全连接神经网络的输入输出是基本一致的。\n",
    "\n",
    "卷积神经网络与全连接神经网络的唯一区别在于神经网络中相邻两层的连接方式\n",
    "\n",
    "- 在全连接神经网络中，每相邻两层之间的节点都有**边**相连，一般会将每一层全连接层中的节点组织成一列，以便显示连接结构。\n",
    "- 而对于卷积神经网络，相邻两层之间只有**部分节点**相连，为了展示每一层神经元的维度，一般会将每一层卷积层的节点组织成一个**三维矩阵**。\n",
    "\n",
    "为什么使用卷积神经网络代替全连接神经网络？\n",
    "\n",
    "使用全连接神经网络处理图像的最大问题在于全连接层的参数太多。对于MNIST数据，每一张图片的大小是28×28×l，其中28×28 为图片的大小，×l表示图像是黑白的，只有一个色彩通道。假设第一层隐藏层的节点数为500个，那么一个全链接层的州经网络将有28×28×500+500 =392500 个参数。同理当图片的尺寸更大，通道数更多时，采用全连接的方式会导致参数过多，从而产生过拟合的情况，因此需要一个更合理的神经网络结构来有效地减少神经网络中参数个数。卷积神经网络就可以达到这个目的。\n",
    "\n",
    "在卷积神经网络的前几层中，每一层的节点都被组织成一个三维矩阵。图2中虚线部分展示了卷积神经网络的一个连接示意图，从图中可以看出卷积神经网络中前几层中每一个节点只和上一层中部分的节点相连。\n",
    "\n",
    "一个**卷积神经网络**主要由以下**5种结构**组成：\n",
    "\n",
    "- **输入层**：输入层是整个神经网络的输入， 在处理图像的卷积神经网络中，它一般代表了一张图片的像素矩阵。比如在图2中， 最左侧的三维矩阵就可以代表一张图片。其中三维矩阵的长和宽代表了图像的大小，而三维矩阵的深度代表了图像的色彩通道( **channel** ）。比如黑白图片的深度为1，而在RGB色彩模式下，图像的深度为3 。从输入层开始， 卷积神经网络通过不同的神经网络结构将**上一层的三维矩阵**转化为**下一层的三维矩阵**， 直到最后的全连接层。\n",
    "\n",
    "- **卷积层**：卷积层是一个卷积神经网络中最为重要的部分。和传统全连接层不同， 卷积层中每一个节点的输入只是上一层神经网络的一小块，这个小块常用的大小有3 × 3 或者5 × 5 。卷积层试图将神经网络中的每一小块进行**更加深入**地分析从而得到抽象程度更高的特征。一般来说，通过卷积层处理过的节点矩阵会**变得更深**，所以在图2中可以看到经过卷积层之后的节点矩阵的**深度会增加**。\n",
    "- **池化层**：池化层神经网络不会改变三维矩阵的深度，但是它可以**缩小矩阵的大小**。池化操作可以认为是将一张分辨率较高的图片转化为分辨率较低的图片。通过池化层，可以进一步缩小最后全连接层中节点的个数，从而达到减少整个神经网络中参数的目的。\n",
    "- **全连接层**：如图2所示，在经过多轮卷积层和池化层的处理之后，在卷积神经网络的最后一般会是由1~2个全连接层来给出最后的分类结果。经过几轮卷积层和池化层的处理之后，可以认为图像中的信息已经被抽象成了信息含量更高的特征。我们可以将卷积层和池化层看成自动图像特征提取的过程。在特征提取完成之后，仍然需要使用全连接层来完成分类任务。\n",
    "- **Softmax层**：So位max层主要用于**分类问题**。通过Softmax层，可以得到当前样例属于不同种类的概率分布情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**卷积层与池化层**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "定义输入矩阵\n",
    "'''\n",
    "M = np.array([\n",
    "        [[1],[-1],[0]],\n",
    "        [[-1],[2],[1]],\n",
    "        [[0],[2],[-2]]\n",
    "    ])\n",
    "print(\"Matrix shape is: \",M.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "M=\\left(\\begin{array}{ccc}{1} & {-1} & {0} \\\\ {-1} & {2} & {1} \\\\ {0} & {2} & {-2}\\end{array}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "定义卷积过滤器, 深度为1\n",
    "'''\n",
    "filter_weight = tf.get_variable('weights', [2, 2, 1, 1], initializer = tf.constant_initializer([\n",
    "                                                                        [1, -1],\n",
    "                                                                        [0, 2]]))\n",
    "biases = tf.get_variable('biases', [1], initializer = tf.constant_initializer(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "W=\\left(\\begin{array}{cc}{1} & {-1} \\\\ {0} & {2}\\end{array}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "调整输入的格式符合TensorFlow的要求\n",
    "'''\n",
    "M = np.asarray(M, dtype='float32')\n",
    "M = M.reshape(1, 3, 3, 1)\n",
    "\n",
    "'''\n",
    "计算矩阵通过卷积层过滤器和池化层过滤器计算后的结果\n",
    "'''\n",
    "x = tf.placeholder('float32', [1, None, None, 1])\n",
    "conv = tf.nn.conv2d(x, filter_weight, strides = [1, 2, 2, 1], padding = 'SAME')\n",
    "bias = tf.nn.bias_add(conv, biases)\n",
    "pool = tf.nn.avg_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    convoluted_M = sess.run(bias,feed_dict={x:M})\n",
    "    pooled_M = sess.run(pool,feed_dict={x:M})\n",
    "    \n",
    "    print(\"convoluted_M: \\n\", convoluted_M)\n",
    "    print(\"pooled_M: \\n\", pooled_M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算法实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LeNet—5模型**\n",
    "\n",
    "LeNet-5 模型是Yann LeCun 教授于1998 年在论文[***Gradient-Based Learning Applied to Document Recognition***](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf*)中提出的，LeNet-5 模型可以达到大约99.2% 的正确率。LeNet-5模型总共有7层，图3展示了LeNet-5 模型的架构。\n",
    "\n",
    "第一层，卷积层\n",
    "\n",
    "> 这一层的输入就是原始的图像像素， LeNet-5 模型接受的输入层大小为32 x 32 x l 。第一个卷积层过滤器的尺寸为 5 × 5 ，深度为6 ，不使用全0填充，步长为1 。因为没有使用全0填充，所以这一层的输出的尺寸为32-5+1=28 ， 深度为6 。这一个卷积层总共有5x5x1x6+6=156 个参数，其中6个为偏置项参数。因为下一层的节点矩阵有28 x28x6=4704个节点，每个节点和5 × 5=25 个当前层节点相连，所以本层卷积层总共有4704 x(25 + 1) = 122304 个连接。\n",
    "\n",
    "第二层，池化层\n",
    "\n",
    "> 这一层的输入为第一层的输出， 是一个28x28x6 的节点矩阵。本层采用的过滤器大小为2 × 2 ，长和宽的步长均为2，所以本层的输出矩阵大小为14 × 14 × 6 。\n",
    "\n",
    "第三层，卷积层\n",
    "\n",
    "> 本层的输入矩阵大小为14 × 14 × 6 ，使用的过滤器大小为5x5，深度为16 。本层不使用全0填充， 步长为1。本层的输出矩阵大小为10 x 10 × 16 。按照标准的卷积层，本层应该有5x5x6x16+16=2416个参数，10×10x16x(25+1) =41600个连接。\n",
    "\n",
    "第四层，池化层\n",
    "\n",
    "> 本层的输入矩阵大小为10 x 10 x 16 ，采用的过滤器大小为2× 2 ，步长为2 。本层的输出矩阵大小为5×5x16 。\n",
    "\n",
    "第五层，全连接层\n",
    "\n",
    "> 本层的输入矩阵大小为5×5x16 ，在LeNet-5模型的论文中将这一层称为卷积层，但是因为过滤器的大小就是5x5，所以和全连接层没有区别，在之后的TensorFlow程序实现中也会将这一层看成全连接层。本层的输出节点个数为120 ，总共有sxsx16x120+120=48120 个参数。\n",
    "\n",
    "第六层，全连接层\n",
    "\n",
    "> 本层的输入节点个数为120个，输出节点个数为84 个，总共参数为120x84+84=10164个。\n",
    "\n",
    "第七层，全连接层\n",
    "\n",
    "> 本层的输入节点个数为84个，输出节点个数为10个，总共参数为84×10+10=850个。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算法验证与评价"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**推理阶段**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "设定神经网络的参数\n",
    "'''\n",
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "IMAGE_SIZE = 28\n",
    "NUM_CHANNELS = 1\n",
    "NUM_LABELS = 10\n",
    "\n",
    "CONV1_DEEP = 32\n",
    "CONV1_SIZE = 5\n",
    "\n",
    "CONV2_DEEP = 64\n",
    "CONV2_SIZE = 5\n",
    "\n",
    "FC_SIZE = 512\n",
    "\n",
    "'''\n",
    "定义前向传播的过程\n",
    "'''\n",
    "def inference(input_tensor, train, regularizer):\n",
    "    with tf.variable_scope('layer1-conv1'):\n",
    "        conv1_weights = tf.get_variable(\n",
    "            \"weight\", [CONV1_SIZE, CONV1_SIZE, NUM_CHANNELS, CONV1_DEEP],\n",
    "            initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv1_biases = tf.get_variable(\"bias\", [CONV1_DEEP], initializer=tf.constant_initializer(0.0))\n",
    "        conv1 = tf.nn.conv2d(input_tensor, conv1_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))\n",
    "\n",
    "    with tf.name_scope(\"layer2-pool1\"):\n",
    "        pool1 = tf.nn.max_pool(relu1, ksize = [1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "\n",
    "    with tf.variable_scope(\"layer3-conv2\"):\n",
    "        conv2_weights = tf.get_variable(\n",
    "            \"weight\", [CONV2_SIZE, CONV2_SIZE, CONV1_DEEP, CONV2_DEEP],\n",
    "            initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv2_biases = tf.get_variable(\"bias\", [CONV2_DEEP], initializer=tf.constant_initializer(0.0))\n",
    "        conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))\n",
    "\n",
    "    with tf.name_scope(\"layer4-pool2\"):\n",
    "        pool2 = tf.nn.max_pool(relu2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        pool_shape = pool2.get_shape().as_list()\n",
    "        nodes = pool_shape[1] * pool_shape[2] * pool_shape[3]\n",
    "        reshaped = tf.reshape(pool2, [pool_shape[0], nodes])\n",
    "\n",
    "    with tf.variable_scope('layer5-fc1'):\n",
    "        fc1_weights = tf.get_variable(\"weight\", [nodes, FC_SIZE],\n",
    "                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer != None: tf.add_to_collection('losses', regularizer(fc1_weights))\n",
    "        fc1_biases = tf.get_variable(\"bias\", [FC_SIZE], initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "        fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases)\n",
    "        if train: fc1 = tf.nn.dropout(fc1, 0.5)\n",
    "\n",
    "    with tf.variable_scope('layer6-fc2'):\n",
    "        fc2_weights = tf.get_variable(\"weight\", [FC_SIZE, NUM_LABELS],\n",
    "                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer != None: tf.add_to_collection('losses', regularizer(fc2_weights))\n",
    "        fc2_biases = tf.get_variable(\"bias\", [NUM_LABELS], initializer=tf.constant_initializer(0.1))\n",
    "        logit = tf.matmul(fc1, fc2_weights) + fc2_biases\n",
    "\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**训练阶段**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "定义神经网络相关的参数\n",
    "'''\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE_BASE = 0.01\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 6000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "\n",
    "'''\n",
    "定义训练过程\n",
    "'''\n",
    "def train(mnist):\n",
    "    # 定义输出为4维矩阵的placeholder\n",
    "    x = tf.placeholder(tf.float32, [\n",
    "            BATCH_SIZE,\n",
    "            LeNet5_infernece.IMAGE_SIZE,\n",
    "            LeNet5_infernece.IMAGE_SIZE,\n",
    "            LeNet5_infernece.NUM_CHANNELS],\n",
    "        name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, LeNet5_infernece.OUTPUT_NODE], name='y-input')\n",
    "    \n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    y = LeNet5_infernece.inference(x,False,regularizer)\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # 定义损失函数、学习率、滑动平均操作以及训练过程。\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        mnist.train.num_examples / BATCH_SIZE, LEARNING_RATE_DECAY,\n",
    "        staircase=True)\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "        \n",
    "    # 初始化TensorFlow持久化类。\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            reshaped_xs = np.reshape(xs, (\n",
    "                BATCH_SIZE,\n",
    "                LeNet5_infernece.IMAGE_SIZE,\n",
    "                LeNet5_infernece.IMAGE_SIZE,\n",
    "                LeNet5_infernece.NUM_CHANNELS))\n",
    "            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: reshaped_xs, y_: ys})\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                print(\"After %d training step(s), loss on training batch is %g.\" % (step, loss_value))\n",
    "                \n",
    "'''\n",
    "主函数入口\n",
    "''' \n",
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets(\"../../../datasets/MNIST_data\", one_hot=True)\n",
    "    train(mnist)\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**用于图片分类问题的卷积神经网络架构：**\n",
    "\n",
    "> **输入层 —>（卷积层 ＋ —> 池化层？）＋ —> 全连接层＋**\n",
    "\n",
    "其中“卷积层+”表示一层或者多层卷积层；“池化层？”表示表示没有或者一层池化层；在多轮卷积层和池化层之后，卷积神经网络在输出之前一般会经过1～2 个全连接层 (比如，**LeNet-5**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TensorFlow实战 Google 深度学习框架**\n",
    ">\n",
    "> [**LeNet-5详解与实现**](<https://www.charleychai.com/blogs/2018/ai/NN/lenet.html>)\n",
    ">\n",
    "> [**Gradient-Based Learning Applied to Document Recognition**](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)\n",
    ">\n",
    "> [**李宏毅深度学习**](<http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML17.html>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
